---
title:      PyTorch中模型的存储与加载
date:       2019-04-22
categories: 'PyTorch'
description: 演示PyTorch中模型的存储与加载方法。
tags:
    - 深度学习
    - PyTorch
updated: 
music-id: 
---
## 简介
- 本案例使用简单神经网络显示如何保存模型以及数据批处理。


## 步骤
- 生成训练数据集
	- 还是使用回归时生成的数据集
	- 代码
		- ```python
			import torch
			import matplotlib.pyplot as plt
			%matplotlib inline
			torch.manual_seed(2019)
			x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)
			y = x.pow(2) + 0.2*torch.rand(x.size())                 
			# 绘制散点图显示数据点分布
			plt.scatter(x.data.numpy(), y.data.numpy())
			plt.show()
			```
	- 看一下训练数据分布
		- ![](/asset/2019-04-22/data.png)
- 搭建神经网络
	- 代码
		- ```python
			import torch.nn as nn
			
			class Net(nn.Module):
				def __init__(self):
					super(Net, self).__init__()
					self.hidden = nn.Sequential(
						nn.Linear(1, 10),
						nn.ReLU(),
						nn.Linear(10, 1)
						)
				def forward(self, x):
					return self.hidden(x) 
			```
- 简单训练神经网络
	- 代码
		- ```python
			net1 = Net()
			optimizer = torch.optim.Adam(net1.parameters(), lr=0.001)
			loss_func = torch.nn.MSELoss()
			
			losses = []
			for epoch in range(100):
				pred = net1(x)
				loss = loss_func(pred, y)
				
				losses.append(loss.data.numpy())
				
				optimizer.zero_grad()
				loss.backward()
				optimizer.step()
			
			plt.plot([i for i in range(100)], losses)
			plt.show()
			```
- **保存并提取训练完的模型**
	- 代码
		- ```python
			pred1 = net1(x[:10])
			print("pred1",pred1)
			torch.save(net1, 'my_model.pkl')
			net2 = torch.load('my_model.pkl')
			pred2 = net2(x[:10])
			print("pred2", pred2)
			```
	- 演示效果
		- 可以看到，模型的效果是一致的保存是成功的。
		- ![](/asset/2019-04-22/rst1.png)
- **保存并提取训练完模型的参数**
	- 很多时候，模型保存很费空间，只保存模型参数，下次使用将参数加载进未训练的新模型对象即可。
	- 代码
		- ```python
			torch.save(net1.state_dict(), 'params_model.pkl')
			net3 = Net()
			net3.load_state_dict(torch.load('params_model.pkl'))
			print(net1(x[:10]).data.numpy())
			print(net3(x[:10]).data.numpy())
			```
	- 演示效果
		- 可以看到，预测结果依然没有改变。
		- ![](/asset/2019-04-22/rst2.png)
- **批量数据训练**
	- 代码
		- ```python
			from torch.utils.data import Dataset, TensorDataset, DataLoader
			dataset = TensorDataset(torch.unsqueeze(torch.Tensor(x), dim=1), torch.unsqueeze(torch.Tensor(y), dim=1))
			loader = DataLoader(
				dataset=dataset,      
				batch_size=32,
				shuffle=True,
				num_workers=2,
			)
			net = Net()
			loss_func = nn.MSELoss()
			optimizer = torch.optim.Adam(net.parameters(), lr=0.001)
			losses = []
			for epoch in range(100):
				# 每次取出一批数据训练
				for step, (batch_x, batch_y) in enumerate(loader):
					print(batch_x.shape)
					pred = net(batch_x)
					loss = loss_func(pred, batch_y)
					losses.append(loss.data.numpy())
					optimizer.zero_grad()
					loss.backward()
					optimizer.step()
					
			plt.plot([i for i in range(100)], losses)
			plt.show()
			```
	- 演示效果
		- 可以看到，对于批量提取器，当数据不足以满足一批时会取出剩余的所有数据。
		- 训练效果
			- ![](/asset/2019-04-22/rst3.png)
			- 图片波动大是因为每一批取出了损失。


## 补充说明
- 本案例使用PyTorch框架，如果你是神经网络的新手建议你使用这个框架，上手容易，网络搭建结构化。本类框架案例均用代码和效果说话，关于神经网络的原理可以见我的其他博客。
- 具体代码见[我的Github](https://github.com/luanshiyinyang/Tutorial/tree/Pytorch/StoreAndBatch)，欢迎star或者fork。（开发环境为Jupyter）
- 博客同步至[个人博客网站](https://luanshiyinyang.github.io)，欢迎查看。